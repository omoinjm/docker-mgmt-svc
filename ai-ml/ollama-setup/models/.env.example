# Ollama Models Configuration Example

# Service name and container configuration
SERVICE_NAME=ollama-models
CONTAINER_NAME=ollama-models

# API server port
OLLAMA_HOST=0.0.0.0:11434

# Volume mount for persistent model storage
MODEL_VOLUME_NAME=ollama-models

# Default model to load on startup (customize as needed)
DEFAULT_MODEL=deepseek-coder
# Available options: deepseek-coder, deepseek-r1, llama2, gemma, mistral

# Optional: Auto-pull additional models on startup
# AUTO_PULL_MODELS=llama2,gemma

# Optional: GPU acceleration (if you have NVIDIA Container Toolkit installed)
# OLLAMA_GPU=cuda

# Optional: Number of parallel requests
# OLLAMA_NUM_PARALLEL=2

# Optional: Thread count
# OLLAMA_NUM_THREAD=8

# Optional: Context window size (tokens)
# OLLAMA_NUM_CTX=4096

# Optional: Debug logging
# OLLAMA_DEBUG=false

# Optional: Keep-alive timeout (in minutes)
# OLLAMA_KEEP_ALIVE=5m

# Optional: Model cache size (for GPU memory)
# OLLAMA_MAX_VRAM=24gb

# Optional: Temperature for model responses (0-2)
# OLLAMA_TEMPERATURE=0.7

# Optional: Logging
# OLLAMA_LOG_LEVEL=info
