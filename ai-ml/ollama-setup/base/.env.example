# Ollama Base Image Configuration Example

# Service name and container configuration
SERVICE_NAME=ollama
CONTAINER_NAME=ollama

# API server port
OLLAMA_HOST=0.0.0.0:11434

# Optional: GPU acceleration (if you have NVIDIA Container Toolkit installed)
# OLLAMA_GPU=cuda

# Optional: Number of parallel requests
# OLLAMA_NUM_PARALLEL=4

# Optional: Thread count
# OLLAMA_NUM_THREAD=8

# Optional: Debug logging
# OLLAMA_DEBUG=false

# Optional: Keep-alive timeout (in minutes)
# OLLAMA_KEEP_ALIVE=5m

# Optional: Model cache size
# OLLAMA_MAX_VRAM=24gb

# Optional: Logging
# OLLAMA_LOG_LEVEL=info
