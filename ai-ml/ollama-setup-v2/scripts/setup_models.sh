#!/bin/bash

# Ollama Model Setup Script
# This script automatically pulls models on container startup

set -e

echo "=========================================="
echo "Ollama Model Setup Script"
echo "=========================================="
echo ""

# Wait for Ollama server to be ready
echo "‚è≥ Waiting for Ollama server to be ready..."
max_attempts=30
attempt=0

while [ $attempt -lt $max_attempts ]; do
  if curl -s http://localhost:11434 > /dev/null 2>&1; then
    echo "‚úÖ Ollama server is ready!"
    break
  fi
  attempt=$((attempt + 1))
  echo "  Attempt $attempt/$max_attempts... (waiting 2s)"
  sleep 2
done

if [ $attempt -eq $max_attempts ]; then
  echo "‚ùå Timeout: Ollama server did not start in time"
  exit 1
fi

echo ""
echo "üì¶ Pulling models..."
echo ""

# Chat & Reasoning Models
echo "‚Üí Pulling reasoning model: deepseek-r1:8b"
ollama pull deepseek-r1:8b

echo "‚Üí Pulling reasoning model: deepseek-r1:70b (large)"
ollama pull deepseek-r1:70b

# General Purpose Models
echo "‚Üí Pulling general model: llama3.1:8b"
ollama pull llama3.1:8b

echo "‚Üí Pulling general model: llama3.1:70b (large)"
ollama pull llama3.1:70b

# Text Generation Models
echo "‚Üí Pulling text model: llama2:7b"
ollama pull llama2:7b

echo "‚Üí Pulling text model: llama2:70b (large)"
ollama pull llama2:70b

echo ""
echo "=========================================="
echo "‚úÖ All models pulled successfully!"
echo "=========================================="
echo ""
echo "Available models:"
ollama list
echo ""
echo "Ollama server is running on 0.0.0.0:11434"
echo "API endpoint: http://localhost:11434"
echo ""

# Keep container alive
exec "$@"
