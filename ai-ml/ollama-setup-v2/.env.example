# Ollama V2 Configuration

# Server Settings
TZ=UTC

# Ollama Server Configuration
# OLLAMA_HOST is configured in docker-compose.yml as 0.0.0.0:11434
# This makes the API accessible on all network interfaces

# Model Management
# Models are stored in the ollama_data volume at /root/.ollama/models
# Use the setup_models.sh script to automatically pull models on startup
# Or use: docker-compose exec ollama ollama pull <model-name>

# Available Models to Pull (examples in scripts/setup_models.sh):
# - deepseek-r1:8b, deepseek-r1:70b (reasoning models)
# - llama3.1:8b, llama3.1:70b (general purpose)
# - llama2:7b, llama2:70b (text generation)
# - mistral:7b (efficient)
# - neural-chat:7b (optimized for chat)

# API Usage
# Access Ollama API at: http://localhost:11434
# List models: curl http://localhost:11434/api/tags
# Generate response: curl -X POST http://localhost:11434/api/generate \
#   -d '{"model":"llama2","prompt":"Hello"}'

# Performance Tuning (Optional)
# For limited resources, use smaller models (8b variants)
# For better quality, use larger models (70b variants)
# Models take significant disk space and memory

# GPU Support
# To enable GPU acceleration:
# 1. Ensure Docker has GPU support configured
# 2. Add gpus: all to docker-compose.yml services section
# 3. Ollama will automatically use CUDA/ROCm if available
